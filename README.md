# Terminology

- Fine Tuning - Adjusting a pre-trained model on a specific dataset to specialize its performance for a particular task.
- RHLF - (Reinforcement Learning from Human Feedback): A training method where a model learns to perform tasks by optimizing against a reward signal derived from human feedback, rather than fixed datasets.
- GenAI Project LifeCycle - The complete series of stages from initial conception and data gathering, through model training and evaluation, to deployment and ongoing monitoring of a generative AI system.
- Artificial Intelligence (AI): The simulation of human intelligence in machines that are programmed to think and learn like humans.
- Machine Learning (ML): A subset of AI that involves training a machine to learn from data and make decisions or predictions based on it without being explicitly programmed.
- Deep Learning: A subset of machine learning that uses neural networks with multiple layers (deep networks) to analyze various factors of data.
- Neural Networks: Computational models inspired by the human brain, composed of layers of nodes or "neurons" that process data and learn complex patterns.
- Generative Adversarial Networks (GANs): A class of machine learning frameworks where two neural networks contest with each other in a game (generative versus discriminative) to generate new, synthetic instances of data that can pass for real data.
- Transformer Models: A type of neural network architecture that relies heavily on self-attention mechanisms to process sequential data (such as text) and is effective for a wide range of tasks, including translation and summarization.
- BERT (Bidirectional Encoder Representations from Transformers): A transformer-based machine learning technique for natural language processing pre-training that enables a model to understand context from both left and right sides of a token within text.
- Autocomplete: A feature of language models that predicts the next word or sequence of words in a sentence based on the previous context.
- Natural Language Processing (NLP): A field of AI focused on enabling computers to understand, interpret, and produce human language content.
- Sequence-to-Sequence Models (Seq2Seq): Models that transform a given sequence of elements, such as words in a sentence, into another sequence, used primarily for translation and summarization.
- Backpropagation: The fundamental algorithm used for training neural networks, where errors are calculated and propagated backwards through the network to adjust the weights.
- Overfitting: A modeling error which occurs when a function is too closely fitted to a limited set of data points and fails to generalize to new data.
- Underfitting: Occurs when a model is too simple to learn the underlying pattern of the data and therefore performs poorly on training data and new data.
- Supervised Learning: A type of machine learning where the model is trained on a labeled dataset, meaning the data includes answers (labels) for training.
- Unsupervised Learning: Learning from test data that has not been labeled, categorized, or classified, where the algorithms must instead identify patterns in the data.
- Reinforcement Learning: An area of machine learning concerned with how software agents ought to take actions in an environment to maximize some notion of cumulative reward.
- Prompt Engineering: The art of crafting inputs or prompts that guide a generative AI model to produce the desired output effectively.
- Tokenization: The process of converting text into a series of tokens or smaller pieces that can be more easily processed by AI models.
- Few-Shot Learning: A type of machine learning where the model learns to perform a task proficiently from a very limited amount of training data.
- Zero-Shot Learning: An approach where a model applies what it has learned from one task to perform on completely new, unseen tasks without any specific training.
- Transfer Learning: Leveraging a pre-trained model (trained on a large dataset) to perform similar but distinct tasks, reducing the need for extensive new data and computation.
- Language Model: An AI model that understands and generates human-like text based on the context it has learned from vast amounts of training data.
- Autoregressive Model: A type of model that generates sequences, where each new output element is dependent on the preceding elements.
- Catastropic Forgetting:
- 4C's -
- PEFT
- 
